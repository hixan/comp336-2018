{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "In this assignment you will implement a non-trivial problem that processes Big Data. To facilitate its processing in a regular computer, the actual amount of data will not be big, but the techniques that you will implement would scale to larger volumes of data.\n",
    "\n",
    "This assignment is worth 15% of the total assessment of the unit.\n",
    "\n",
    "This assignment relates to the following Learning Outcomes:\n",
    "* Apply Map-reduce techniques to a number of problems that involve Big Data.\n",
    "* Apply Big Data techniques to data mining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission deadline: Friday Week 12, 11:55pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code unzips the data stored in tweets.zip. This is the same data you used in Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "if not Path('10000 tweets-NEW.json').exists():\n",
    "    print(\"Unzipping tweets\")\n",
    "    with zipfile.ZipFile('cleaned-tweets.zip') as myzip:\n",
    "        myzip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a [Python generator](https://wiki.python.org/moin/Generators) that simulates a stream of tweets. You will use this iterator in some of the following tasks. The function uses the `yield` statement instead of a `return` statement so that it does not need to read the entire file into memory. By doing this, the function can work with files of  unlimited size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def stream_tweets():\n",
    "    with open('10000 tweets-NEW.json', encoding='iso8859-1') as jfile:\n",
    "        for line in jfile:\n",
    "            try:\n",
    "                next_tweet = json.loads(line)\n",
    "            except:\n",
    "                continue # yield next tweet instead of returning str\n",
    "            yield next_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a demonstration of the use of this code in the lectures and workshops. Below is an example of how it can be used in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'tag:search.twitter.com,2005:715690137900941312', 'objectType': 'activity', 'actor': {'objectType': 'person', 'id': 'id:twitter.com:18064228', 'link': 'http://www.twitter.com/Intelledox', 'displayName': 'Intelledox', 'postedTime': '2008-12-11T23:47:55.000Z', 'image': 'https://pbs.twimg.com/profile_images/485981380585603072/inMuMtJ7_normal.png', 'summary': \"Intelledox's mobile-ready digitalization software helps over 1 million people to do business faster, smarter & efficiently Digitalize your business process now!\", 'links': [{'href': 'http://www.intelledox.com', 'rel': 'me'}], 'friendsCount': 486, 'followersCount': 549, 'listedCount': 24, 'statusesCount': 1188, 'twitterTimeZone': 'Canberra', 'verified': False, 'utcOffset': '39600', 'preferredUsername': 'Intelledox', 'languages': ['en'], 'location': {'objectType': 'place', 'displayName': 'Canberra, Australia'}, 'favoritesCount': 55}, 'verb': 'post', 'postedTime': '2016-04-01T00:00:00.000Z', 'generator': {'displayName': 'HubSpot', 'link': 'http://www.hubspot.com/'}, 'provider': {'objectType': 'service', 'displayName': 'Twitter', 'link': 'http://www.twitter.com'}, 'link': 'http://twitter.com/Intelledox/statuses/715690137900941312', 'body': 'Register for #Convergence2016 to hear@ChelleMelbourne talk about how to manage change through digital transformation https://t.co/7pxwwDeaXm', 'object': {'objectType': 'note', 'id': 'object:search.twitter.com,2005:715690137900941312', 'summary': 'Register for #Convergence2016 to hear@ChelleMelbourne talk about how to manage change through digital transformation https://t.co/7pxwwDeaXm', 'link': 'http://twitter.com/Intelledox/statuses/715690137900941312', 'postedTime': '2016-04-01T00:00:00.000Z'}, 'favoritesCount': 0, 'twitter_entities': {'hashtags': [{'text': 'Convergence2016', 'indices': [13, 29]}], 'urls': [{'url': 'https://t.co/7pxwwDeaXm', 'expanded_url': 'http://hubs.ly/H02w_rn0', 'display_url': 'hubs.ly/H02w_rn0', 'indices': [117, 140]}], 'user_mentions': [], 'symbols': []}, 'twitter_filter_level': 'low', 'twitter_lang': 'en', 'retweetCount': 0, 'gnip': {'matching_rules': [{'value': 'bio_location: \"Australia\"', 'tag': None}, {'value': 'bio_location: \"Canberra\"', 'tag': None}], 'urls': [{'url': 'https://t.co/7pxwwDeaXm', 'expanded_url': 'http://www.cvent.com/events/convergence-2016-optimising-your-organisational-change/agenda-77418453122540c5aa767dd59702ef91.aspx', 'expanded_status': 200}], 'klout_score': 40, 'language': {'value': 'en'}, 'profileLocations': [{'objectType': 'place', 'geo': {'type': 'point', 'coordinates': [149.12807, -35.28346]}, 'address': {'country': 'Australia', 'countryCode': 'AU', 'locality': 'Canberra', 'region': 'Australian Capital Territory'}, 'displayName': 'Canberra, Australian Capital Territory, Australia'}]}}\n",
      "{'id': 'tag:search.twitter.com,2005:715690143449899009', 'objectType': 'activity', 'actor': {'objectType': 'person', 'id': 'id:twitter.com:188921458', 'link': 'http://www.twitter.com/losebabyweight1', 'displayName': 'losebabyweight', 'postedTime': '2010-09-09T22:40:10.000Z', 'image': 'https://pbs.twimg.com/profile_images/522696965503455233/WfF2aJ7N_normal.png', 'summary': 'http://www.losebabyweight.com.au offers mums safe and proven plans to lose weight. Lose an average of 1kg a week.', 'links': [{'href': 'http://www.losebabyweight.com.au', 'rel': 'me'}], 'friendsCount': 218, 'followersCount': 1960, 'listedCount': 17, 'statusesCount': 14439, 'twitterTimeZone': 'Sydney', 'verified': False, 'utcOffset': '39600', 'preferredUsername': 'losebabyweight1', 'languages': ['en'], 'location': {'objectType': 'place', 'displayName': 'Australia'}, 'favoritesCount': 0}, 'verb': 'post', 'postedTime': '2016-04-01T00:00:01.000Z', 'generator': {'displayName': 'Facebook', 'link': 'http://www.facebook.com/twitter'}, 'provider': {'objectType': 'service', 'displayName': 'Twitter', 'link': 'http://www.twitter.com'}, 'link': 'http://twitter.com/losebabyweight1/statuses/715690143449899009', 'body': 'CONGRATULATIONS Suzie Walker on both your beautiful little man and your FANTASTIC commitment and hard work. You... https://t.co/m4QLVq0BTr', 'object': {'objectType': 'note', 'id': 'object:search.twitter.com,2005:715690143449899009', 'summary': 'CONGRATULATIONS Suzie Walker on both your beautiful little man and your FANTASTIC commitment and hard work. You... https://t.co/m4QLVq0BTr', 'link': 'http://twitter.com/losebabyweight1/statuses/715690143449899009', 'postedTime': '2016-04-01T00:00:01.000Z'}, 'favoritesCount': 0, 'twitter_entities': {'hashtags': [], 'urls': [{'url': 'https://t.co/m4QLVq0BTr', 'expanded_url': 'http://fb.me/WUTD9TnQ', 'display_url': 'fb.me/WUTD9TnQ', 'indices': [115, 138]}], 'user_mentions': [], 'symbols': []}, 'twitter_filter_level': 'low', 'twitter_lang': 'en', 'retweetCount': 0, 'gnip': {'matching_rules': [{'value': 'bio_location: \"Australia\"', 'tag': None}], 'urls': [{'url': 'https://t.co/m4QLVq0BTr', 'expanded_url': 'https://www.facebook.com/photo.php?fbid=1257229527624685', 'expanded_status': 403}], 'klout_score': 44, 'language': {'value': 'en'}, 'profileLocations': [{'objectType': 'place', 'geo': {'type': 'point', 'coordinates': [135, -25]}, 'address': {'country': 'Australia', 'countryCode': 'AU'}, 'displayName': 'Australia'}]}}\n",
      "{'id': 'tag:search.twitter.com,2005:715690141306650624', 'objectType': 'activity', 'actor': {'objectType': 'person', 'id': 'id:twitter.com:97578801', 'link': 'http://www.twitter.com/wantirnaweather', 'displayName': 'Wantirna Weather', 'postedTime': '2009-12-18T02:34:57.000Z', 'image': 'https://pbs.twimg.com/profile_images/580069194/Eclipse_003vsm_normal.jpg', 'summary': 'Personal Weather Station from Wantirna, Victoria, Australia.', 'links': [{'href': 'http://www.vic-weather.info', 'rel': 'me'}], 'friendsCount': 28, 'followersCount': 80, 'listedCount': 12, 'statusesCount': 66248, 'twitterTimeZone': 'Melbourne', 'verified': False, 'utcOffset': '39600', 'preferredUsername': 'wantirnaweather', 'languages': ['en'], 'location': {'objectType': 'place', 'displayName': 'Victoria, Australia'}, 'favoritesCount': 1}, 'verb': 'post', 'postedTime': '2016-04-01T00:00:01.000Z', 'generator': {'displayName': 'Weather Display Tweet', 'link': 'http://www.weather-display.com'}, 'provider': {'objectType': 'service', 'displayName': 'Twitter', 'link': 'http://www.twitter.com'}, 'link': 'http://twitter.com/wantirnaweather/statuses/715690141306650624', 'body': 'Wantirna, VIC, AU 11:00 AM Temp 19.8Â°C, RH 67pct, Winds NNW @ 0.0 km/h, Rain Today  0 mm, 1014.3 hpa &amp; Steady. #vicweather', 'object': {'objectType': 'note', 'id': 'object:search.twitter.com,2005:715690141306650624', 'summary': 'Wantirna, VIC, AU 11:00 AM Temp 19.8Â°C, RH 67pct, Winds NNW @ 0.0 km/h, Rain Today  0 mm, 1014.3 hpa &amp; Steady. #vicweather', 'link': 'http://twitter.com/wantirnaweather/statuses/715690141306650624', 'postedTime': '2016-04-01T00:00:01.000Z'}, 'favoritesCount': 0, 'twitter_entities': {'hashtags': [{'text': 'vicweather', 'indices': [115, 126]}], 'urls': [], 'user_mentions': [], 'symbols': []}, 'twitter_filter_level': 'low', 'twitter_lang': 'en', 'retweetCount': 0, 'gnip': {'matching_rules': [{'value': 'bio_location: \"Australia\"', 'tag': None}], 'klout_score': 28, 'language': {'value': 'en'}, 'profileLocations': [{'objectType': 'place', 'geo': {'type': 'point', 'coordinates': [145, -37]}, 'address': {'country': 'Australia', 'countryCode': 'AU', 'region': 'Victoria'}, 'displayName': 'Victoria, Australia'}]}}\n",
      "{'id': 'tag:search.twitter.com,2005:715690146062950400', 'objectType': 'activity', 'actor': {'objectType': 'person', 'id': 'id:twitter.com:3266593548', 'link': 'http://www.twitter.com/OnAussie', 'displayName': \"What's On Aussie\", 'postedTime': '2015-07-03T02:09:37.000Z', 'image': 'https://pbs.twimg.com/profile_images/616796684706672640/vprn5HK4_normal.jpg', 'summary': 'Connecting local communities with their people, news, businesses and events.', 'links': [{'href': 'http://WhatsOnAussie.com.au', 'rel': 'me'}], 'friendsCount': 60, 'followersCount': 37, 'listedCount': 11, 'statusesCount': 946, 'twitterTimeZone': None, 'verified': False, 'utcOffset': None, 'preferredUsername': 'OnAussie', 'languages': ['en'], 'location': {'objectType': 'place', 'displayName': 'Australia'}, 'favoritesCount': 84}, 'verb': 'post', 'postedTime': '2016-04-01T00:00:02.000Z', 'generator': {'displayName': 'Facebook', 'link': 'http://www.facebook.com/twitter'}, 'provider': {'objectType': 'service', 'displayName': 'Twitter', 'link': 'http://www.twitter.com'}, 'link': 'http://twitter.com/OnAussie/statuses/715690146062950400', 'body': 'So much to see and do when you Visit Central Australia\\n\"We have our own kind of beaches in the #RedCentreNT! The... https://t.co/eRyP7eTk9X', 'object': {'objectType': 'note', 'id': 'object:search.twitter.com,2005:715690146062950400', 'summary': 'So much to see and do when you Visit Central Australia\\n\"We have our own kind of beaches in the #RedCentreNT! The... https://t.co/eRyP7eTk9X', 'link': 'http://twitter.com/OnAussie/statuses/715690146062950400', 'postedTime': '2016-04-01T00:00:02.000Z'}, 'favoritesCount': 0, 'twitter_entities': {'hashtags': [{'text': 'RedCentreNT', 'indices': [95, 107]}], 'urls': [{'url': 'https://t.co/eRyP7eTk9X', 'expanded_url': 'http://fb.me/4tbRKQpgN', 'display_url': 'fb.me/4tbRKQpgN', 'indices': [116, 139]}], 'user_mentions': [], 'symbols': []}, 'twitter_filter_level': 'low', 'twitter_lang': 'en', 'retweetCount': 0, 'gnip': {'matching_rules': [{'value': 'bio_location: \"Australia\"', 'tag': None}], 'urls': [{'url': 'https://t.co/eRyP7eTk9X', 'expanded_url': 'https://www.facebook.com/photo.php?fbid=1022848134476636', 'expanded_status': 403}], 'klout_score': 32, 'language': {'value': 'en'}, 'profileLocations': [{'objectType': 'place', 'geo': {'type': 'point', 'coordinates': [135, -25]}, 'address': {'country': 'Australia', 'countryCode': 'AU'}, 'displayName': 'Australia'}]}}\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for s in stream_tweets():\n",
    "    if counter > 3:\n",
    "        break\n",
    "    counter += 1\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 marks)\n",
    "Fill the gaps in the class below that processes the stream and issues the following standing queries:\n",
    "\n",
    "* (1 mark) The length of the shortest tweet and the length of the longest tweet so far.\n",
    "* (2 marks) The twitter ID of the person who has posted most tweets in the last 1000 posts.\n",
    "* (2 marks) The twitter ID of the most active twitter when we apply an exponentially decaying window with $c=10^{-3}$ and a threshold of 0.5.\n",
    "\n",
    "In your implementation, make sure that the system scales well to unlimited streams, and answer the following question:\n",
    "\n",
    "1. How much memory do you need to reserve to keep the information about each of the standing queries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: this question needs fixing\n",
    "part 3, make the algorithm better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "class StreamProcessor:\n",
    "    \n",
    "    shortest = None\n",
    "    longest = None\n",
    "    frequency = []\n",
    "    active = Counter()\n",
    "    \n",
    "    def _step_rval(self):\n",
    "        return {'shortest': self.shortest,\n",
    "               'longest': self.longest,\n",
    "               'most_frequent': max(Counter(self.frequency)),\n",
    "               'most_active': max(self.active, key=lambda x: self.active[x])}\n",
    "    \n",
    "    # end of class variables\n",
    "    def step(self, item):\n",
    "        \"\"\"Process one item from the stream and return the answers to the\n",
    "        standing queries as a Python dictionary with the following keys:\n",
    "          - shortest\n",
    "          - longest\n",
    "          - most_frequent\n",
    "          - most_active\n",
    "        \"\"\"\n",
    "        userid = item['actor']['id']\n",
    "        post = item['body']\n",
    "        l = len(post)\n",
    "        self.frequency.append(userid)\n",
    "        self.frequency = self.frequency[-1000:] # cut off entries before last 1000.\n",
    "        \n",
    "        # apply decay to all users\n",
    "        # TODO: find mathematical equivalent of this decay in growth\n",
    "        # so dont have to iterate over whole counter, only increment the growth indicator\n",
    "        # (IE each time add a larger number, and occasionally when numbers get too big \n",
    "        # multiply down to managable size, and reset growth indicator)\n",
    "        for k in self.active:\n",
    "            self.active[k] = self.active[k]*.999 # (1-10**(-3))\n",
    "        # add this tweet to activity\n",
    "        self.active[userid] += 1\n",
    "        \n",
    "        # initialize lowest and highest\n",
    "        if self.shortest is None:\n",
    "            self.shortest = l\n",
    "            self.longest = l\n",
    "            return self._step_rval() # no more calculation\n",
    "        \n",
    "        if l<self.shortest:\n",
    "            self.shortest = l\n",
    "        if l > self.longest:\n",
    "            self.longest = l\n",
    "        \n",
    "        return self._step_rval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will apply the stream processor to the first 5 elements of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shortest': 140, 'longest': 140, 'most_frequent': 'id:twitter.com:18064228', 'most_active': 'id:twitter.com:18064228'}\n",
      "{'shortest': 138, 'longest': 140, 'most_frequent': 'id:twitter.com:188921458', 'most_active': 'id:twitter.com:188921458'}\n",
      "{'shortest': 127, 'longest': 140, 'most_frequent': 'id:twitter.com:97578801', 'most_active': 'id:twitter.com:97578801'}\n",
      "{'shortest': 127, 'longest': 140, 'most_frequent': 'id:twitter.com:97578801', 'most_active': 'id:twitter.com:3266593548'}\n",
      "{'shortest': 127, 'longest': 140, 'most_frequent': 'id:twitter.com:97578801', 'most_active': 'id:twitter.com:225568917'}\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "stream = StreamProcessor()\n",
    "for s in stream_tweets():\n",
    "    if s == 'Tweet error':\n",
    "        continue\n",
    "    if counter >= 5:\n",
    "        break\n",
    "    counter += 1\n",
    "    print(stream.step(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (5 marks)\n",
    "Apply the minhashing techniques we have covered in week 7 to determine the set of near-duplicates among the tweet posts. For this exercise use only the first 500 tweet posts (so that you do not need to wait too long). To complete this assignment you can reuse code from the lecture notebooks and from the workshop exercises. Use your judgement to determine the parameters and answer the following questions:\n",
    "\n",
    "1. What value of $k$ did you use to represent the $k$-shingles and why?\n",
    "2. Did you hash the $k$-shingles and why?\n",
    "3. If you hashed the $k$-shingles, how many buckets did you use and why?\n",
    "4. How many hashes did you use for minhashing, how many buckets, and why?\n",
    "5. How many bands and rows did you use for locality-sensitive hashing and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, count\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "# implementation:\n",
    "# k_shingles function creates a zip of k copies of the tweet, each\n",
    "# offset by one more then the previous and returns the columns. EG:\n",
    "#          >>> k_shingles('testing', 3)\n",
    "# internally:  v v v v v\n",
    "#comprehension t e s t i\n",
    "#   function   e s t i n\n",
    "#   returns    s t i n g\n",
    "# zip transposes\n",
    "# map applies the hashing function\n",
    "#          <<< {'tes','est','sti','tin','ing'} \n",
    "# \n",
    "# note: zip only returns values while arguments have elements at the\n",
    "# current index.\n",
    "# \n",
    "# this method of computing k_shingles is memory intensive, but has\n",
    "# a lower time complexity (and is very pythonic). The memory shouldnt\n",
    "# be an issue as tweets are all below 280 characters (so (280-k) * k)\n",
    "def k_shingles(tweet, k, h=lambda x:''.join(x)):\n",
    "    if len(tweet) < k:\n",
    "        return tweet\n",
    "    else:\n",
    "        return set(map(h, zip(*[tweet[i:len(tweet)-(k-i-1)] for i in range(k)])))\n",
    "\n",
    "# print(k_shingles('0123456789', 3))\n",
    "# print(k_shingles('test', 3, h=hash))\n",
    "# print(k_shingles('tesa', 3, h=hash))\n",
    "\n",
    "# for easier debugging\n",
    "Tweet = namedtuple('Tweet', ['body','id'])\n",
    "\n",
    "tweetset = {t['actor']['id']:Tweet(t['body'], t['actor']['id']) for _, t in zip(range(50), stream_tweets())}\n",
    "#tweetset = {i: Tweet(a, i) for i, a in enumerate(['abc','abcd','zeqr', 'abacus', 'c', 'abcde'])}\n",
    "buckets = 2**64\n",
    "k = 20\n",
    "hash_count = 40\n",
    "# creates functions of the form hash_function(1) = lambda s:hash(s+str(1))%buckets\n",
    "#                             # hash_function(1)('test') = some integer between 0 and buckets.\n",
    "hash_function = lambda i: lambda s: hash(str(s)+str(i))%buckets\n",
    "# print(hash_function(3)('a'))\n",
    "hash_functions = [hash_function(i) for i in range(hash_count)]\n",
    "# define signature as a named tuple (instead of an anonymous tuple)\n",
    "# helps for knowing what objects are (as python is duck typed)\n",
    "Signature = namedtuple('Signature', ['id', 'hash_values'])\n",
    "\n",
    "def similarity(signature1, signature2):\n",
    "    similar = 0\n",
    "    for v1, v2 in zip(signature1.hash_values, signature2.hash_values):\n",
    "        if v1 == v2:\n",
    "            similar += 1\n",
    "    return similar / hash_count\n",
    "\n",
    "# basically a constructor for the Signature namedtuple\n",
    "def signature(tweet, k=k, hash_functions=hash_functions):\n",
    "    return Signature(tweet.id, tuple(min(map(h, k_shingles(tweet.body, k))) for h in hash_functions))\n",
    "        # does not return a generator of the signature as it may need to be accessed many times.\n",
    "\n",
    "# signatures = [signature(v) for k, v in tweetset.items()]\n",
    "\n",
    "def signature_sensitivity_hashing(signature, row_size=3, hash_function=hash_functions[0]):\n",
    "    bands = len(signature.hash_values)//row_size\n",
    "    new_hash_values = tuple([hash_function(signature.hash_values[i*row_size:(i+1)*row_size]) for i in range(bands)] +\n",
    "                                    [hash_function(signature.hash_values[:-len(signature.hash_values)%bands])])\n",
    "    # sometimes the second slice will return [:0] meaning the same hash is always returned\n",
    "    # (when hash_count%row_size==0)\n",
    "    # this is accounted for below:\n",
    "    return Signature(signature.id, new_hash_values\n",
    "                     [:-1 if len(signature.hash_values)%row_size==0 else len(new_hash_values)])\n",
    "\n",
    "# a = signature_sensitivity_hashing(signatures[0], row_size=10)\n",
    "# b = signatures[0]\n",
    "# print(len(a.hash_values), len(b.hash_values)/10)\n",
    "\n",
    "def similarity_sets(tweetset, k, hash_functions, row_count, similarity_cutoff, precalc_similarities=None):\n",
    "    # when similarity cutoff < 0; this will essentially s\n",
    "    hash_count = len(hash_functions)\n",
    "    signatures = [signature_sensitivity_hashing(signature_sensitivity_hashing(signature(tweet,\n",
    "                                                                                        k=k,\n",
    "                                                                                        hash_functions=hash_functions),\n",
    "                                                                              row_size=row_count,\n",
    "                                                                              hash_function=hash_functions[0]))\n",
    "                  for i, tweet in tweetset.items()]\n",
    "\n",
    "    # create lookup table of similarities (and dont repeat calculation if it was provided)\n",
    "    if precalc_similarities is None:\n",
    "        sims = {key1 : {key2 : None for key2 in tweetset} for key1 in tweetset}\n",
    "        for s1, s2 in combinations(signatures, 2):\n",
    "            sim = similarity(s1, s2)\n",
    "            sims[s1.id][s2.id] = sim\n",
    "            sims[s2.id][s1.id] = sim # it is a symmetric matrix\n",
    "    else:\n",
    "        sims = precalc_similarities\n",
    "        \n",
    "    if similarity_cutoff < 0:\n",
    "        return [set(tweetset.keys())], sims\n",
    "    sets = []\n",
    "    for tweet in tweetset:\n",
    "        added=False\n",
    "        for s in sets:\n",
    "            for t in s:\n",
    "                # min clustering\n",
    "                if sims[tweet][t] > similarity_cutoff:\n",
    "                    s.add(tweet)\n",
    "                    added=True # go on to the next tweet\n",
    "                    break\n",
    "            if added:\n",
    "                break\n",
    "        if not added:\n",
    "            sets.append(set([tweet]))\n",
    "    return sets, sims\n",
    "        \n",
    "    \n",
    "    \n",
    "# for t1, t2 in combinations([signature(v) for k, v in tweetset.items()], 2):\n",
    "#     sim = similarity(t1, t2)\n",
    "#     count = 0\n",
    "#     if sim > 0:\n",
    "#         print(t1.id, t2.id, sim)\n",
    "\n",
    "# sets, similarities = similarity_sets(tweetset, k=20, hash_functions=hash_functions,\n",
    "#                                      row_count=2, similarity_cutoff=.1)[0]\n",
    "\n",
    "# some experimentation\n",
    "hash_functions = [hash_function(i) for i in range(20)] # 50 hashing functions\n",
    "df = pd.DataFrame(columns=['k','row_count','cutoff','groups'])\n",
    "for k in range(3, 10):\n",
    "    k*= 3\n",
    "    for row_count in range(1, 5):\n",
    "        prev_calc = None\n",
    "        for similarity_cutoff in range(10):\n",
    "            similarity_cutoff/=10\n",
    "            sets, prev_calc = similarity_sets(tweetset, k=k, hash_functions=hash_functions,\n",
    "                                         row_count=row_count, similarity_cutoff=similarity_cutoff,\n",
    "                                         precalc_similarities=prev_calc)\n",
    "            df = df.append({'k':float(k), 'row_count':float(row_count), 'cutoff':similarity_cutoff, 'groups':sets}, ignore_index=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd815a459b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_group_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'groups'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['max_group_length'] = list(map(lambda x:max(map(len, x)), df['groups'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (5 marks)\n",
    "Implement a MapReduce version of PageRank **using combiners** as described in the lectures of week 9. The MapReduce version should incorporate teleporting with $\\beta=0.85$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will use Python's built-in functions `map` and `reduce`. For example, the following code is a Python version that uses MapReduce to compute the sum of squares of the numbers in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def my_square(x):\n",
    "    return x**2\n",
    "\n",
    "def my_sum(x,y):\n",
    "    return x+y\n",
    "\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "def mapreduce(a_list):\n",
    "    temp = map(my_square, a_list) # Note that map returns an iterator, not a list\n",
    "    return reduce(my_sum, temp)\n",
    "\n",
    "mapreduce(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2**2+3**2+4**2+5**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Python code is not efficient and it does not take advantage of parallel computing units (feel free to search the Web for parallel versions) but it will serve for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `map` returns a Python iterator and not a list and there are operations that cannot be performed on it. For example, you cannot select a slice or compute the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b960b2a81dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    }
   ],
   "source": [
    "temp = map(my_square, my_list)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e910bd635dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "temp = map(my_square, my_list)\n",
    "print(temp[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, use an artificially generated network such as the one used in the workshop of week 9. The code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(n, sparsity):\n",
    "    \"Return a transition matrix with n nodes\"\n",
    "    # Fill the matrix\n",
    "    result = np.zeros((n,n))\n",
    "    for i in range(int(n*n - n*n*sparsity)):\n",
    "        x = np.random.randint(n)\n",
    "        y = np.random.randint(n)\n",
    "        result[x,y] = 1\n",
    "        \n",
    "    # Normalise the results\n",
    "    for c in range(n):\n",
    "        degree = np.sum(result[:, c])\n",
    "        if degree > 0:\n",
    "            result[:, c] /= degree\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , 0. , 1. ],\n",
       "       [0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.5, 0. , 0. , 0.5, 0. ],\n",
       "       [0. , 0. , 0. , 0.5, 0. ],\n",
       "       [0.5, 1. , 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_network(5,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In your demonstration, generate a network with 20 nodes and compute the PageRank of each node. \n",
    "* Your solution must include a graph that shows how the PageRank changes at each iteration.\n",
    "* Do not attempt to remove dead ends (to simplify this exercise).\n",
    "* What size of blocks did you use for your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M = generate_network(20, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does not use MapReduce (it's based on the lecture notebook). Use it for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-80e34070b58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mPR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpage_countt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moldPR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epsylon = 0.0001\n",
    "beta = 0.85\n",
    "page_count = 5\n",
    "M = generate_network(page_count, 0.7)\n",
    "PR = np.ones((pages, 1)) / page_countt\n",
    "iterations = 0\n",
    "oldPR = np.zeros((pages,1))\n",
    "allPR = [PR]\n",
    "while max(np.abs(oldPR-PR)) > epsylon:\n",
    "    oldPR = PR\n",
    "    PR = beta*(np.dot(M, PR)) + (1-beta)/pages*np.ones((pages,1))\n",
    "    allPR.append(PR)\n",
    "    iterations += 1\n",
    "print(\"PR after %i iterations:\" % iterations)\n",
    "print(PR)\n",
    "for p in range(pages):\n",
    "    data = [onePR[p,0] for onePR in allPR]\n",
    "    plt.plot(data)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"PageRank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the solution with MapReduce below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (0.030000000000000006, ()))\n",
      "(3, (0.085, ()))\n",
      "(4, (0.085, ()))\n",
      "(1, (0.030000000000000006, (3, 4)))\n",
      "(4, (0.17, ()))\n",
      "(2, (0.030000000000000006, (4,)))\n",
      "(1, (0.085, ()))\n",
      "(3, (0.085, ()))\n",
      "(3, (0.030000000000000006, (1, 3)))\n",
      "(3, (0.17, ()))\n",
      "(4, (0.030000000000000006, (3,)))\n",
      "<map object at 0x7fe54e893c50>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-2af5ebfef53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mmapreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# epsylon beta page_count M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-2af5ebfef53e>\u001b[0m in \u001b[0;36mmapreduce\u001b[0;34m(values, map_function, reduce_function)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_pagerank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-2af5ebfef53e>\u001b[0m in \u001b[0;36mr\u001b[0;34m(*x, **xx)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-2af5ebfef53e>\u001b[0m in \u001b[0;36mreduce_pagerank\u001b[0;34m(key_values, constants)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moutlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpagerank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutlinks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpagerank\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "Constants = namedtuple('Constants', ['epsillon','beta', 'page_count'])\n",
    "c = Constants(.0001, .85, len(M))\n",
    "\n",
    "def tuplify(f):\n",
    "    def r(*x, **xx):\n",
    "        return tuple(f(*x, **xx))\n",
    "    return r\n",
    "\n",
    "@tuplify\n",
    "def map_pagerank(key_value, constants=c):\n",
    "    key, value = key_value\n",
    "#     print('map:\\n\\tkey: {}\\n\\tvalue: {}'.format(key, value))\n",
    "    # input------------------------------------\n",
    "    # key: (page, pagerank)\n",
    "    # value: itterable of outgoing links\n",
    "    for v in value:\n",
    "#         print('map return:\\n\\tkey: {}\\n\\tvalue: {}'.format(v, (key[1] / len(value) * constants.beta, ())))\n",
    "        yield v, (key[1] / len(value) * constants.beta, ()) # transversal\n",
    "#     print('map return:\\n\\tkey: {}\\n\\tvalue: {}'.format(key[0], ((1-constants.beta) / constants.page_count, value)))\n",
    "    yield (key[0],\n",
    "    ((1-constants.beta) / constants.page_count, value)) # teleportation\n",
    "    # output-----------------------------------\n",
    "    # key: page\n",
    "    # value: (pagerank, outgoing links)\n",
    "    \n",
    "@tuplify\n",
    "def reduce_pagerank(key_values, constants=c):\n",
    "    key, values = key_values\n",
    "#     print('reduce:\\n\\tkey: {}\\n\\tvalues: {}'.format(key, values))\n",
    "    # input-----------------------------------\n",
    "    # key: page\n",
    "    # values: itterable of (pagerank, outgoing links)s\n",
    "    outlinks = []\n",
    "    pagerank = 0\n",
    "    for pr, ol in values:\n",
    "        outlinks += list(ol)\n",
    "        pagerank += pr\n",
    "    yield (key, pagerank), tuple(outlinks)\n",
    "    # output-----------------------------------\n",
    "    # key: (page, pagerank)\n",
    "    # value: tuple of outgoing links\n",
    "\n",
    "@tuplify\n",
    "def prepare_reduce(key_value_pairs, constants=c):\n",
    "    # function takes all key value pairs and groups them by key.\n",
    "    rval = {}\n",
    "    for key, value in key_value_pairs:\n",
    "        if key not in rval:\n",
    "            rval[key] = set()\n",
    "        rval[key].add(value)\n",
    "    for key in rval:\n",
    "        yield key, tuple(rval[key])\n",
    "\n",
    "@tuplify\n",
    "def prepare_map(network_matrix):\n",
    "    for pagenumber, row in enumerate(network_matrix):\n",
    "        yield (pagenumber, 1/len(network_matrix)), tuple(i for i in range(len(network_matrix)) if row[i] != 0)\n",
    "@tuplify\n",
    "def mapchain(function, args):\n",
    "    return chain(*map(function, args))\n",
    "\n",
    "def mapreduce(values, map_function=map_pagerank, reduce_function=reduce_pagerank):\n",
    "    d = values\n",
    "    d = mapchain(map_pagerank, d)\n",
    "    for v in d:\n",
    "        print(v)\n",
    "    d = map(reduce_pagerank, d)\n",
    "    print(d)\n",
    "    for v in d:\n",
    "        print(v)\n",
    "\n",
    "mapreduce(prepare_map(M))\n",
    "\n",
    "# epsylon beta page_count M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_map\n",
      "\t 2 ((0, 0.2), ())\n",
      "\t 2 ((1, 0.2), (3, 4))\n",
      "\t 2 ((2, 0.2), (4,))\n",
      "\t 2 ((3, 0.2), (1, 3))\n",
      "\t 2 ((4, 0.2), (3,))\n",
      "map_pagerank\n",
      "\t 2 (0, (0.030000000000000006, ()))\n",
      "\t 2 (3, (0.085, ()))\n",
      "\t 2 (4, (0.085, ()))\n",
      "\t 2 (1, (0.030000000000000006, (3, 4)))\n",
      "\t 2 (4, (0.17, ()))\n",
      "\t 2 (2, (0.030000000000000006, (4,)))\n",
      "\t 2 (1, (0.085, ()))\n",
      "\t 2 (3, (0.085, ()))\n",
      "\t 2 (3, (0.030000000000000006, (1, 3)))\n",
      "\t 2 (3, (0.17, ()))\n",
      "\t 2 (4, (0.030000000000000006, (3,)))\n",
      "prepare_reduce\n",
      "\t 2 (0, ((0.030000000000000006, ()),))\n",
      "\t 2 (3, ((0.030000000000000006, (1, 3)), (0.17, ()), (0.085, ())))\n",
      "\t 2 (4, ((0.17, ()), (0.085, ()), (0.030000000000000006, (3,))))\n",
      "\t 2 (1, ((0.085, ()), (0.030000000000000006, (3, 4))))\n",
      "\t 2 (2, ((0.030000000000000006, (4,)),))\n",
      "reduce_pagerank\n",
      "\t 2 ((0, 0.030000000000000006), ())\n",
      "\t 2 ((3, 0.28500000000000003), (1, 3))\n",
      "\t 2 ((4, 0.28500000000000003), (3,))\n",
      "\t 2 ((1, 0.11500000000000002), (3, 4))\n",
      "\t 2 ((2, 0.030000000000000006), (4,))\n"
     ]
    }
   ],
   "source": [
    "@tuplify\n",
    "# m function calls ${function} on all arguments in *args and chains them together (so it is one list of results)\n",
    "def mc(function, *args):\n",
    "    return mapchain(function, *args)\n",
    "print('prepare_map')\n",
    "d = prepare_map(M)\n",
    "for e in d:\n",
    "    print('\\t', len(e) , e)\n",
    "print('map_pagerank')\n",
    "d = m(map_pagerank, prepare_map(M))\n",
    "for v in d:\n",
    "    print('\\t', len(v), v)\n",
    "d = prepare_reduce(d)\n",
    "print('prepare_reduce')\n",
    "for v in d:\n",
    "    print('\\t', len(v), v)\n",
    "d = m(reduce_pagerank, d)\n",
    "print('reduce_pagerank')\n",
    "for v in d:\n",
    "    print('\\t', len(v), v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
